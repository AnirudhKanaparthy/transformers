{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing `torch` takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import DataSplit\n",
    "from model import TransformerDecoder\n",
    "from train import train_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "dataset_path = 'data/tinyshakespeare.txt'\n",
    "percent_train = 0.90\n",
    "\n",
    "batch_size = 16\n",
    "context_length = 64\n",
    "\n",
    "n_embd = 64\n",
    "n_layer = 4\n",
    "num_head = 4\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 3e-4\n",
    "max_iters = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "eval_intervals = 100\n",
    "eval_iters = 250\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running on device: cpu, cuda version: 11.8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Running on device: {device}, cuda version: {torch.version.cuda}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and Reading the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path, 'r') as fp:\n",
    "    dataset = fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "Creating a vocabulary. Essentially contains all the *tokens* which is recognised by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(list(set(dataset)))\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders and Decoder\n",
    "We are using a simple method to encode and decode the text data into numbers (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(vocabulary)}\n",
    "itos = {i: ch for i, ch in enumerate(vocabulary)}\n",
    "\n",
    "def encode(x): return [stoi[ch] for ch in x]\n",
    "def decode(x): return ''.join([itos[i] for i in x])\n",
    "\n",
    "test_text = 'This is a sample sentence.'\n",
    "assert (test_text == decode(encode(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding All the Dataset\n",
    "We encode all the dataset into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = torch.tensor(encode(dataset), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "We split the dataset to train the model and validate the model. Validation is important as we are trying to generalise the model so that it can produce sentences which are \"like\" the training data but not exactly the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(encoded_dataset) * percent_train)\n",
    "train_data, val_data = encoded_dataset[:idx], encoded_dataset[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split: DataSplit) -> torch.Tensor:\n",
    "    return train_data if split == DataSplit.TRAIN else val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoder(\n",
    "    vocabulary_size=vocab_size,\n",
    "    embedding_dim=n_embd,\n",
    "    context_length=context_length,\n",
    "    number_of_layers=n_layer,\n",
    "    number_of_heads=num_head,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[   1/1000], Training Loss:  4.359576, Validation Loss:  4.362085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 102/1000 [00:20<14:48,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 101/1000], Training Loss:  3.114107, Validation Loss:  3.148436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 203/1000 [00:33<10:44,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 201/1000], Training Loss:  2.788194, Validation Loss:  2.800481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [00:46<11:58,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 301/1000], Training Loss:  2.644121, Validation Loss:  2.656755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [00:51<00:31, 19.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 401/1000], Training Loss:  2.586380, Validation Loss:  2.586755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 502/1000 [01:11<08:20,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 501/1000], Training Loss:  2.552685, Validation Loss:  2.551725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 603/1000 [01:23<05:12,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 601/1000], Training Loss:  2.521989, Validation Loss:  2.523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 702/1000 [01:36<04:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 701/1000], Training Loss:  2.498847, Validation Loss:  2.498564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 802/1000 [01:48<03:38,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 801/1000], Training Loss:  2.483111, Validation Loss:  2.479725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 902/1000 [02:01<01:24,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[ 901/1000], Training Loss:  2.459799, Validation Loss:  2.466308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:06<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss:\n",
      "\tTraining:  2.459799\n",
      "\tValidation:  2.466308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_losses = train_transformer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    get_data=get_data,\n",
    "    batch_size=batch_size,\n",
    "    context_length=context_length,\n",
    "    maximum_iterations=max_iters,\n",
    "    eval_intervals=eval_intervals,\n",
    "    eval_iterations=eval_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mord bnoferry were shild tachirchnd y medy wheintollle,\n",
      "Bag auncoou of se\n",
      "Thy.\n",
      "Fou, w; m;\n",
      "HOr so mmom akthy inorshesenemenovelbee f gor difspr Bicdas at be?\n",
      "\n",
      "Wht s os Loe hautost LEAveanord my sofloe bpsels batyi thits th asa tethaigtsint Pm r, t leeg ssaa-d,\n",
      "Gethece want\n",
      "Mad, toresg ber mareagald.\n",
      "We, ament tousp todthaut r , heast walavershiwof sn gadathinnd s at! Vnnond,\n",
      "Th\n",
      "Angewnt, wouou,, at, texses tithave b llanfanCache, h ce cathss eap cnge,\n",
      "Bnge, che sot aichotom strystwtor;ilde k nete. arencerd ssthe rr go beaireat.\n",
      "\n",
      "\n",
      "Srend ano chak! hasr.\n",
      "TUNUSThan f sfiewat tamarst t:\n",
      "\n",
      "\n",
      "MKYVKosks:\n",
      "Andind, the hiy willinthastors tat iciK:\n",
      "Hingen ct d Dhik?\n",
      "ERh gocmouretlis n3Lomes cofegeartres wamy ou; aych f hethoncou, Fth wilithighe\n",
      "sito m.\n",
      "Q  il sthe brstoveal hyo stinchecanetraNETon f aves gothtoorsiur teem.\n",
      "W widez she ier, it ofsnd slo cendo thienk,\n",
      "Anger f rd: t ad'pead hato ondse.\n",
      "Thithes ghe IThind t\n",
      "Tomiche thou t aanarshaprrs\n",
      "Towe doune G Foof he b wicos w. thesor-iltoot dove chef\n"
     ]
    }
   ],
   "source": [
    "generate_next_tokens = 100\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generation = decode(model.generate(idx, max_next_tokens=generate_next_tokens)[0].tolist())\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_losses\u001b[49m[DataSplit\u001b[38;5;241m.\u001b[39mTRAIN])),\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: all_losses[DataSplit\u001b[38;5;241m.\u001b[39mTRAIN],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: all_losses[DataSplit\u001b[38;5;241m.\u001b[39mVALIDATION]\n\u001b[0;32m      5\u001b[0m })\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Melt the DataFrame to create a \"long-form\" DataFrame\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_melted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmelt(df, id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m'\u001b[39m], value_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m                     var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_type\u001b[39m\u001b[38;5;124m'\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'iter': range(len(all_losses[DataSplit.TRAIN])),\n",
    "    'train': all_losses[DataSplit.TRAIN],\n",
    "    'val': all_losses[DataSplit.VALIDATION]\n",
    "})\n",
    "\n",
    "# Melt the DataFrame to create a \"long-form\" DataFrame\n",
    "df_melted = pd.melt(df, id_vars=['iter'], value_vars=['train', 'val'],\n",
    "                    var_name='loss_type', value_name='loss')\n",
    "\n",
    "# Create the plot\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_melted, x='iter', y='loss', hue='loss_type')\n",
    "\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(title='Loss Type')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
